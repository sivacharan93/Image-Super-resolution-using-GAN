# -*- coding: utf-8 -*-
"""SRGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WYOIRUUTXVJlbDq-H_TlGWAJTz0bMu2h
"""

#Importing required libraries
import os
import numpy as np
import pickle
import time
import matplotlib.pyplot as plt
from keras.applications.vgg19 import VGG19
from keras.models import Model
from keras.optimizers import Adam
import keras.backend as K
from keras.layers import Input
from keras.models import Model
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.core import Flatten
from keras.layers.normalization import BatchNormalization
from keras.layers import Dense
from keras.layers.core import Activation
from keras.layers.advanced_activations import LeakyReLU, PReLU
from keras import initializers
from keras.layers import add
from keras import losses

np.random.seed(10)
image_shape = (400,400,3)

#Loading the data from pickle files
path = str('/home/sivac/Project/')


f = open(path + 'train_lr.pkl','rb') 
train_lr = pickle.load(f)

f = open(path + 'train_hr.pkl','rb') 
train_hr = pickle.load(f)

f = open(path + 'val_lr.pkl','rb') 
val_lr = pickle.load(f)

f = open(path + 'val_hr.pkl','rb') 
val_hr = pickle.load(f)

f = open(path + 'test_lr.pkl','rb') 
test_lr = pickle.load(f)


#Function to normalize the input images to [-1,1]
def normalize(input_data):
    img = (np.float32(input_data) - 127.5)/127.5 
    return img
  
#next_batch
def next_batch(x,y,start,batch_size):
     return np.array(normalize(x[start:start + batch_size])),np.array(normalize(y[start:start + batch_size]))

val_lr = normalize(val_lr)
val_hr = normalize(val_hr)

#Defining the generator class

class Generator(object):

    def __init__(self, img_shape):
        
        self.img_shape = img_shape

    def generator(self):
        
        generator_input = Input(shape = self.img_shape)
        model = Conv2D(filters = 64, kernel_size = 9, strides = 1,kernel_initializer = initializers.he_normal(), padding = "same")(generator_input)
        model = PReLU(alpha_initializer='zeros', shared_axes=[1,2])(model)
	    
        skip_input = model
        
        #Residual blocks
        for index in range(16):
            #Residual block
            #Residual for skip connection
            skip_input_1 = model                
            model = Conv2D(filters = 64, kernel_size = 3, kernel_initializer = initializers.he_normal() ,strides = 1, padding = "same")(model)
            model = BatchNormalization(momentum = 0.6)(model)
            model = PReLU(alpha_initializer='zeros', shared_axes=[1,2])(model)
            model = Conv2D(filters = 64, kernel_size = 3,kernel_initializer = initializers.he_normal(), strides = 1, padding = "same")(model)
            model = BatchNormalization(momentum = 0.6)(model)    
            model = add([skip_input_1, model])   

	    
        model = Conv2D(filters = 64, kernel_size = 3, strides = 1,kernel_initializer = initializers.he_normal(), padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = add([skip_input, model])
	    
       #Upsampling blocks
        for index in range(2):
            model = Conv2DTranspose( kernel_size = 3, filters = 256, strides = 2,kernel_initializer = initializers.he_normal(), padding = "same")(model)
            model = LeakyReLU(alpha = 0.25)(model)
	    
	    #Final layer to set the channels to 3 of input image
        model = Conv2D(filters = 3, kernel_size = 9, kernel_initializer = initializers.he_normal(),strides = 1, padding = "same")(model)
        	    #Using tanh activation to get the values in the range of  [-1,1]
        model = Activation('tanh')(model)
	   
        return Model(inputs = generator_input, outputs = model)

#Defining the discriminator class

class Discriminator(object):

    def __init__(self, img_shape):
        
        self.img_shape = img_shape
    
    def discriminator(self):
        
        discriminator_input = Input(shape = self.img_shape)
        
        model = Conv2D(filters = 64, kernel_size = 3,kernel_initializer=initializers.he_normal(), strides = 1, padding = "same")(discriminator_input)
        model = LeakyReLU(alpha = 0.25)(model)        
        
        model = Conv2D(filters = 64, kernel_size = 3, strides = 2, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Conv2D(filters = 128, kernel_size = 3, strides = 1, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Conv2D(filters = 128, kernel_size = 3, strides = 2, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Conv2D(filters = 256, kernel_size = 3, strides = 1, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Conv2D(filters = 256, kernel_size = 3, strides = 2, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Conv2D(filters = 512, kernel_size = 3, strides = 1, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Conv2D(filters = 512, kernel_size = 3, strides = 2, kernel_initializer = initializers.he_normal() ,padding = "same")(model)
        model = BatchNormalization(momentum = 0.6)(model)
        model = LeakyReLU(alpha = 0.25)(model)
        
        model = Flatten()(model)
        model = Dense(1024,kernel_initializer=initializers.he_normal())(model)
        model = LeakyReLU(alpha = 0.25)(model)
       
        model = Dense(1,activation='sigmoid')(model)
        
        return Model(inputs = discriminator_input, outputs = model)
		
#Defining the VGG19 loss function
#https://keras.io/losses/
#https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618
def vgg_loss(y_true, y_pred):
    
    vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=image_shape)
    vgg19.trainable = False
    for layer in vgg19.layers:
        layer.trainable = False
    #Using the 5th block feature maps for loss compliation
    custom_vgg = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)
    custom_vgg.trainable = False
    return losses.mean_squared_error(custom_vgg(y_true),custom_vgg(y_pred))
#Defining the adam optimizer  
def adam_optimizer():
    return  Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)

#Compiling th GAN Network
def gan_combined(discriminator, shape1, generator, optimizer):
    g_input = Input(shape=shape1)
    fake_data = generator(g_input)
    discriminator.trainable = False
    d_output = discriminator(fake_data)
    gan = Model(inputs=g_input, outputs=[fake_data,d_output])
    gan.compile(loss=[vgg_loss, "binary_crossentropy"],
                loss_weights=[1., 1e-3],
                optimizer=optimizer)

    return gan
#Train function for the train process
def train(epochs, batch_size):

    val_error = {'gan':[],'gen':[],'dis':[]}
    iterations = len(train_lr) // batch_size
    input_shape = (100,100,3)
    output_shape = (400,400,3)
    
    generator = Generator(input_shape).generator()
    discriminator = Discriminator(output_shape).discriminator()

    generator.compile(loss=vgg_loss, optimizer=adam_optimizer())
    discriminator.compile(loss="binary_crossentropy", optimizer=adam_optimizer())
    
    gan = gan_combined(discriminator, input_shape, generator, adam_optimizer())

    for epoch in range(1, epochs+1):
        start_time = time.time()
        #iterations = 2
        for itr in range(iterations):
            #print(epoch,itr)
            #Get the batch data
            image_batch_hr,image_batch_lr = next_batch(train_hr,train_lr,itr*batch_size,batch_size)      
                         
            #Pretraining the discriminator before Gan
            #https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3
            images_sr = generator.predict(image_batch_lr)
          
            #Generate labels by samplng from a uniform distribution            
            disc_y = np.concatenate((np.random.uniform(0.80,0.99,batch_size),np.random.uniform(0,0.25,batch_size)))
            disc_input =  np.concatenate((image_batch_hr,images_sr))
            
            #Shuffling the indices
            idx = np.arange(len(disc_input))
            np.random.shuffle(idx)
            disc_y = disc_y[idx]
            disc_input = disc_input[idx]
            
            #Get the discriminator loss
            discriminator.trainable = True
            
            #Compute the Discriminator Loss
            discriminator_loss_total = discriminator.train_on_batch(disc_input, disc_y)
#             discriminator_loss_fake = discriminator.train_on_batch(images_sr, fake_Y)
#             discriminator_loss_total =  discriminator_loss_real + discriminator_loss_fake
            
            
            #Set the GAN Y to high values since we need all the generated images to belong to the input distribution
            gan_Y =  np.random.uniform(0.80,0.99,batch_size)
            #Freeze the discriminator weights before training the GAN
            discriminator.trainable = False
            #Compute the GAN Loss
            gan_loss = gan.train_on_batch(image_batch_lr, [image_batch_hr,gan_Y])
            
       #print("Disc Loss, Loss GAN")
       # print(discriminator_loss_total, gan_loss[0])
       # val_Y =  np.array([0.875]*len(val_lr))
        
       # val_gan_loss,val_gen_loss,val_dis_loss = gan.evaluate(val_lr[:32], [val_hr[:32],val_Y[:32]])
            val_error['gan'].append(gan_loss[0])
            val_error['gen'].append(gan_loss[1])
            val_error['dis'].append(gan_loss[2])
        
        #print(val_error['gan'])
        print("Epoch", str(epoch), "\t| Val Loss =", str(np.mean(val_error['gan'][(epoch-1)*550:(epoch)*550])))


        end_time = time.time()
        print(' Time Taken for this epoch is {} '.format(end_time - start_time))
		#Dumping the loss values for plot later
        with open('error.pkl', 'wb') as fp:
            pickle.dump(val_error, fp)
		#Saving the model for every 5 epochs
        if not epoch%5:
            generator.save('./generator.h5' )

#Calling the train function
train(60,8)

